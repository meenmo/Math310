\documentclass[12pt]{article}
\usepackage{url,amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{array}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{longtable}
\usepackage{graphicx}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\setlength{\oddsidemargin}{.25in}
\setlength{\evensidemargin}{.25in}
\setlength{\textwidth}{6.25in}
\setlength{\topmargin}{-0.4in}
\setlength{\textheight}{8.5in}

\newcommand{\heading}[5]{
   \renewcommand{\thepage}{#1-\arabic{page}}
   \noindent
   \begin{center}
   \framebox{
      \vbox{
    \hbox to 5.78in { {\bf Math/Stat 310: Intro to Mathematical Statistics}
         \hfill #2 }
       \vspace{4mm}
       \hbox to 5.78in { {\Large \hfill #5  \hfill} }
       \vspace{2mm}
       \hbox to 5.78in { {\it #3 \hfill #4} }
      }
   }
   \end{center}
%   \vspace*{4mm}
}

\newcommand{\handout}[3]{\heading{#1}{#2}{Instructor:
Hyunseung Kang}{Scribe: Meenmo Kang}{Lecture #1: #3}}

\setlength{\parindent}{0in}
\setlength{\parskip}{0.1in}

\newcommand{\Var}{{\rm Var}}
\newcommand{\E}{{\rm E}}
\newcommand{\Cov}{{\rm Cov}}
\newcommand{\Corr}{{\rm Corr}}
\newcommand{\pdf}{{\rm pdf}}
\newcommand{\mgf}{{\rm MGF}}
\newcommand{\proof}{{\bf Proof. }} %% To begin a proof write \proof
\newcommand{\qed}{\mbox{}\hspace*{\fill}\nolinebreak\mbox{$\rule{0.6em}{0.6em}$}} %%to end your proof write $\qed$.
\newcommand{\ma}{{\mathcal A}}
\newcommand{\mf}{{\mathcal F}}
\newcommand{\hs}{\heartsuit}
\newcommand{\cs}{\clubsuit}
\newcommand{\noi}{\noindent}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}

\bibliographystyle{plain}

\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\begin{document}
\handout{18}{April 16, 2018}{Sufficiency}
%Set n to the lecture number

\begin{section}{Motivation for Sufficiency}
Statisticians have long been concerned about reducing size of data. This is due to limited computing ability in comparison to size of data at all time. As follows, we are going to see if there is an available function that can efficiently store information of collected sample.\\

In the past, even with a relatively small amount of data, the computing ability was limited. On the other hand, in 2018, we are living with much larger size of data with latest computers. However, as it is called the era of big data, the size of data sets has become massive, so computing ability is still problematic. Therefore, the question how to reduce original data to a simplified function has always been one of the important issues among statisticians. 
\begin{itemize}
	\item Suppose we collect $n$ $iid$ samples $X_i$ $\stackrel{iid}{\sim}$ pdf$_\theta$, to learn about $\theta$.
    \item Is there a function of the $n$ data points, say $T(X_1,...,X_n$), where $T$ captures \textbf{all} the info about $\theta$?
\end{itemize}

\section{Example: Coin Toss}
We want to test whether a coin is fair or not. You flip the coin $n$ times and collect the result $X_1,...,X_n$. Since only two possible outcomes exist, it is binary. 

$$X_i= 
\begin{cases}
1\text{\quad if $i$th toss is head}\\
0\text{\quad if $i$th toss is tail}
\end{cases}$$\\ 

In case of Bernoulli distribution, instead of the entire vector ($X_1,...,X_n$), the proportion of the distribution is enough information to represent the whole sample.
$$T(X_1,...,X_n) = \frac{\sum_{i=1}^n X_i}{n} = \text{Proportion of heads in n tosses}$$

It is noticeable that, through this sufficiency procedure, $n$ data points were summarized as one single number, and there was no loss of information though most of the data was discarded.

\section{Formal Definition of Sufficiency}
A statistic $T(X_1,...,X_n)$ is sufficient for $\theta$ if p$(X_1,...,X_n|T(X_1,...,X_n)))$ is not a function of Q.
\subsection{Factorization Theorem}
A statistic $T(X_1,...,X_n)$ is sufficient for $\theta$ if and only if $$pdf_{\theta}(X_1,...,X_n) 
= g[T(X_1,...,X_n),\theta]\; h(X_1,...,X_n)$$
\subsection{Example(Bernoulli: Coin Toss)}
$(X_1,...,X_n)$ $\stackrel{iid}{\sim}$ Bern($p$) ($p$ is true proportion of heads)\\

\textbf{Goal} \\Learn about p from n data points $(X_1,...,X_n)$.\\
\textbf{Show} \\That we only need $\frac{\sum_{i=1}^n X_i}{n} = T(X_1,...,X_n)$, which is $p$, to learn about $\theta$ (i.e. show T is sufficient)

\subsubsection{By Factorization Theorem}
$$pdf_{\theta}(X_1,...,X_n) = \prod_{i=1}^n[p^{X_i}(1-p)^{1-X_i}] = p^{\sum_{i=1}^n X_i} \cdot (1-p)^{\sum_{i=1}^n (1-X_i)}$$
$$p^{\sum_{i=1}^n X_i} \cdot (1-p)^{n - \sum_{i=1}^n X_i} = p^{nT(X_1,...,X_n)} \cdot (1-p)^{n-nT(X_1,...,X_n)} $$ 
$$g(T(X_1,...,X_n), p) = p^{nT(X_1,...,X_n)} \cdot (1-p)^{n(1-T(X_1,...,X_n))} \qquad h(X_1,...,X_n)=1$$
\newline
Thus $T(X_1,...,X_n) = \frac{\sum_{i=1}^n X_i}{n}$ is sufficient for $\theta$ or (p in this case).

\subsection{Example (Normal)} 
$(X_1,...,X_n)$ $\stackrel{iid}{\sim}$ $N(\mu,\sigma^2)$
$\Rightarrow$What are the sufficient stats for $\mu$ and $\sigma^2$?\\
$$T(X_1,...,X_n) = (\frac{\sum_{i=1}^n X_i}{n}, \frac{\sum_{i=1}^n X_i^2}{n})$$
$$= \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}exp(-\frac{(X_i-\mu)^2}{2\sigma^2}) = (\frac{1}{\sqrt{2\pi\sigma^2}})^n \prod_{i=1}^n exp(-\frac{(X_i-\mu)^2}{2\sigma^2})$$
$$=(\frac{1}{\sqrt{2\pi\sigma^2}})^n \prod_{i=1}^n exp(-\frac{(X_i^2-2\mu X_i+\mu^2)}{2\sigma^2}) = (\frac{1}{\sqrt{2\pi\sigma^2}})^n \prod_{i=1}^n exp(-\frac{X_i^2}{2\sigma^2}-\frac{2\mu X_i}{2\sigma^2} + \frac{\mu^2}{2\sigma^2})$$
$$=(\frac{1}{\sqrt{2\pi\sigma^2}})^n exp(-\sum_{i=1}^n \frac{X_i^2}{2\sigma^2}-\sum_{i=1}^n \frac{2\mu X_i}{2\sigma^2} + \sum_{i=1}^n \frac{\mu^2}{2\sigma^2}) = (\frac{1}{\sqrt{2\pi\sigma^2}})^n exp(-\frac{1}{2\sigma^2}(\sum_{i=1}^n X_i^2-2\mu\sum_{i=1}^n  X_i + n\mu^2))$$
$$=(\frac{1}{\sqrt{2\pi\sigma^2}})^n exp(T(X_1,...,X_n) \cdot [\frac{n\mu}{\sigma^2}, \frac{-n}{2\sigma^2}]) \cdot exp(\frac{-n\mu^2}{2\sigma^2}) \Rightarrow g(T(X_1,...,X_n))$$

Since there is no terms with respect to $X_i$, we assume $h(X_1,...,X_n) =1$.
Therefore, this is sufficient for ($\mu,\sigma^2)$\\


\textbf{\emph{Theorem}}\\
If $T(X_1,...,X_n)$ is sufficient for $\theta$, then your MLE estimation of $\theta$ \textbf{must be} functions of $T(X_1,...,X_n)$.

\textbf{\emph{Proof}} 
$$\hat{\theta}_{MLE} = argmax_{\theta} \; pdf_{\theta}(X_1,...,X_n) $$
$$= argmax_{\theta} \;g(T(X_1,...,X_n), \theta) h(X_1,...,X_n)=argmax_{\theta} \; log[g(T(X_1,...,X_n), p) h(X_1,...,X_n)]$$ $$=argmax_{\theta} (\; log[g(T(X_1,...,X_n), \theta)] + log [h(X_1,...,X_n)]) = argmax_{\theta} \; log[g(T(X_1,...,X_n), \theta)]$$

Hence, since $g$ is only dependent on $T(X_1,...,X_n)$ and $\theta$, every MLE must be a function of $T(X_1,...,X_n)$.

\section{Exponential Family}
A $pdf_\theta$ is a part of a family of functions known as exponential family if the $pdf$ is of the form
$$pdf_\theta \;X_i = exp[c(\theta)\cdot T(x_i)+ d(\theta) + f(x_i)] \; \text{where $c$, $T$, $d$, and $f$ are functions}$$

Note that if $f(x|\theta)$ of $n$ $iid$ samples. $T(x_i)$ is considered to be sufficient as below. 
$$f(x|\theta) = \prod_{i=1}^n pdf(X_i,...,X_n) = \prod_{i=1}^n exp(c(\theta)T(x_i)+ d(\theta) + f(x_i))$$ $$= exp(c(\theta)\sum_{i=1}^n T(x_i)) + n\cdot d(\theta)\cdot exp(\sum_{i=1}^n f(x_i))$$

\subsection{Example: Bernoulli Distribution (Coin Toss)}
Suppose that we flip a coin $n$ times again.
\\n coin toss example (Show its in the exponential family)\\
$$pdf(X_i) = p^{X_i}(1-p)^{1-X_i}$$ 
$$= exp(log(p^{X_i}(1-p)^{1-X_i})) = exp(X_i log(p) + (1-X_i)log(1-p))$$ 
$$=exp(X_i \cdot log(\frac{p}{1-p}) + log(1-p))$$

$$\prod_{i=1}^n pdf(X_i) = \prod_{i=1}^n exp(X_i\cdot log(\frac{p}{1-p}) + log(1-p))$$ 
$$= exp(log(\frac{p}{1-p}) \cdot \sum_{i=1}^n X_i ) + n \cdot log(1-p)$$
$$\Leftrightarrow c(p)=log(\frac{p}{1-p}) \qquad T(X_i)=X_i \qquad d(p)=log(1-p) \qquad f(X_i)=1 $$
Therefore, a coin toss distribution is a part of exponential family.







\end{section}
\end{document}



